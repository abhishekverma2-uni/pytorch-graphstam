{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90817ad0",
   "metadata": {},
   "source": [
    "## M5 Hierarchical Forecasting\n",
    "\n",
    "**This notebook demonstrates Hierarchical Forecasting capability of pytorch-graphstam on the open source M5 Walmart dataset**\n",
    "\n",
    "- The model chosen is GraphSeq2Seq but other model choices (GraphTFT, SimpleGraph) can be substituted as well\n",
    "- While GraphTFT is the most powerful model option, GraphSeq2Seq offers good enough performance for low compute requirements\n",
    "- This non-ensemble version of the model gets a private leaderboard score of ~56 which places it within top-25. With tuning of huber delta parameter for each department, this score can improve to ~55.\n",
    "- For an even higher score <55, average results from 2-3 of these models (trained with varying train/test cutoffs)\n",
    "\n",
    "**Competition Overview**  \n",
    "The M5 Forecasting - Accuracy competition on Kaggle was a large-scale time series forecasting challenge where participants were asked to predict the daily unit sales of various products sold by Walmart over the course of 28 days. This competition is part of the [Makridakis forecasting competitions](https://mofc.unic.ac.cy/m5-competition/) and sought to push the boundaries of accurate demand forecasting.\n",
    "\n",
    "**Objective**  \n",
    "- Forecast the daily sales for thousands of Walmart products across multiple stores.\n",
    "- Achieve accurate forecasts across 12 hierarchy levels\n",
    "\n",
    "**Data Highlights**  \n",
    "- **Historical Sales Data**: Includes daily sales volume for each product, from 2011 to 2016 (roughly 5 years).\n",
    "- **Product & Store Information**: Each item has metadata such as category, department, and store location.\n",
    "- **Calendar & Supplemental Data**: Includes calendar dates, special events, and other features like price changes (snap/holiday flags, promotions, etc.).\n",
    "- The large number of SKUs (items) and diverse store locations introduce complexity and interesting forecasting challenges (e.g., scaling models for many time series, handling different seasonalities, etc.).\n",
    "\n",
    "**Evaluation Metric**  \n",
    "- The competition uses **Weighted Root Mean Squared Scaled Error (WRMSSE)** to evaluate predictions.\n",
    "- Each time series (item/store combination) contributes to the final score based on its assigned weight, ensuring more popular products or categories affect the score more significantly.\n",
    "\n",
    "**Why This Challenge Is Interesting**  \n",
    "- **High-Dimensional Time Series**: Thousands of unique time series, each with its own patterns.\n",
    "- **Hierarchical Forecasting**: Products are aggregated by category, department, and store, offering an opportunity to explore hierarchical or group forecasting methods.\n",
    "\n",
    "**Competition Links**  \n",
    "- [M5 Forecasting - Accuracy (Main Page)](https://www.kaggle.com/competitions/m5-forecasting-accuracy/overview)  \n",
    "- [Data Overview & Download](https://www.kaggle.com/competitions/m5-forecasting-accuracy/data)  \n",
    "- [Evaluation Details](https://www.kaggle.com/competitions/m5-forecasting-accuracy/overview/evaluation)  \n",
    "- [Discussion & FAQs](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891d5cc",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "#### install pytorch >= v2.5.x with CUDA version 12.1, also works with with cu118/cu124/cu128 \n",
    "pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "#### install pytorch-geometric >= v2.6.x\n",
    "check version compatibility with torch & os here: https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html\n",
    "pip install torch_geometric\n",
    "\n",
    "#### install pytorch-geometric dependencies for torch v2.5.* & CUDA v12.1\n",
    "pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
    "    \n",
    "#### install pytorch-graphstam\n",
    "pip install -U git+https://github.com/rsscml/pytorch-graphstam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba424d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core library imports\n",
    "\n",
    "from pytorch_graphstam import graphstam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443ced9",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "A minimally preprocessed dataset can be downloaded from here: https://drive.google.com/file/d/1O75J-8uqxpG7Sd8xv1-7xRnSoc3qrnwO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c085ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming working in current dir\n",
    "df = pd.read_csv(\"m5_processed.csv\")\n",
    "\n",
    "# reduce history to ~3 years to speedup training\n",
    "df = df[df['d']>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec665a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify features to use - only basic features from the dataset are used, please refer to competition docs for more explanation of these features\n",
    "\n",
    "event_cols = ['event_name_1_Christmas',\n",
    "              'event_name_1_Cinco De Mayo',\n",
    "              'event_name_1_ColumbusDay',\n",
    "              'event_name_1_Easter',\n",
    "              \"event_name_1_Father's day\",\n",
    "              'event_name_1_Halloween',\n",
    "              'event_name_1_IndependenceDay',\n",
    "              'event_name_1_LaborDay',\n",
    "              \"event_name_1_Mother's day\",\n",
    "              'event_name_1_NBAFinalsEnd',\n",
    "              'event_name_1_NBAFinalsStart',\n",
    "              'event_name_1_NewYear',\n",
    "              'event_name_1_StPatricksDay',\n",
    "              'event_name_1_SuperBowl',\n",
    "              'event_name_1_Thanksgiving',\n",
    "              'event_name_1_ValentinesDay',\n",
    "              \"event_name_2_Father's day\"] \n",
    "\n",
    "date_cols = ['tm_d','tm_w','tm_m','tm_wm','tm_dw','tm_w_end']\n",
    "\n",
    "snap_ca_col = ['snap_CA']\n",
    "snap_tx_col = ['snap_TX']\n",
    "snap_wi_col = ['snap_WI']\n",
    "snap_cols = ['snap']\n",
    "other_cols = ['sell_price','price_max','price_min','price_mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to determine sample weights\n",
    "\n",
    "def create_weights(df):\n",
    "    # fillna for period > 1941 (forecast period)\n",
    "    df['sales'] = df['sales'].fillna(0)\n",
    "    # normalize wt\n",
    "    df['weight'] = df['weight']/df['weight'].max()\n",
    "    # clip weights\n",
    "    wt_low = np.maximum(df['weight'].quantile(0.05), 0.05)\n",
    "    wt_high = 1.0\n",
    "    df['weight'] = np.clip(df['weight'], a_min=wt_low, a_max=wt_high)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training cutoffs\n",
    "\n",
    "train_till = 1850\n",
    "test_till = 1941\n",
    "\n",
    "# inference cutoffs\n",
    "\n",
    "infer_start = 1942\n",
    "infer_end = 1969\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0b723",
   "metadata": {},
   "source": [
    "#### Build Config for GraphSeq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47d45b",
   "metadata": {},
   "source": [
    "To view complete config details, run the following:\n",
    "- graphstam.show_config_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92523ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features config\n",
    "\n",
    "# Note that the config keys - 'key_combinations', 'lowest_key_combination', 'highest_key_combination' - apply to Hierarchical Forecasting scenarios only\n",
    "# We train for the following 7 levels of hierarchy\n",
    "\n",
    "#L6 ['state_id', 'cat_id']\n",
    "#L7 ['state_id', 'dept_id']\n",
    "#L8 ['store_id', 'cat_id']\n",
    "#L9 ['store_id', 'dept_id']\n",
    "#L10 item_id\n",
    "#L11 ['item_id', 'state_id']\n",
    "#L12 ['item_id', 'store_id']\n",
    "\n",
    "features_config =  {'id_col': 'id',\n",
    "                   'key_combinations':[('store_id','item_id'),  # each of these tuples correspond to a hierarchy level\n",
    "                                       ('store_id','dept_id'),  # To keep computation requirements low enough, restrict to 7 levels\n",
    "                                       ('state_id','item_id'),\n",
    "                                       ('state_id','dept_id'),\n",
    "                                       ('store_id',),\n",
    "                                       ('dept_id',),\n",
    "                                       ('item_id',)\n",
    "                                      ],\n",
    "                   'lowest_key_combination': ('store_id','item_id'), # this is the most granular level - the level at which data is provided\n",
    "                   'highest_key_combination': ('dept_id',),          # this is the topmost level of aggregation\n",
    "                   'target_col': 'sales',\n",
    "                   'time_index_col': 'd',\n",
    "                   'global_context_col_list': ['store_id','item_id','state_id'],\n",
    "                   'static_cat_col_list': ['store_dept_id'],\n",
    "                   'temporal_known_num_col_list': event_cols + date_cols + other_cols + snap_cols,\n",
    "                   'wt_col': 'weight'}\n",
    "\n",
    "# rolling features\n",
    "rolling_features =   [('d','mean', 7, 0),\n",
    "                      ('d','mean', 28, 0),\n",
    "                      ('d','std', 28, 0),\n",
    "                      ('d','mean', 60, 0),\n",
    "                      ('d','std', 60, 0)]\n",
    "\n",
    "# data config\n",
    "data_config = {\n",
    "                \"max_lags\": 91,                             # Max history (lags) for target variable\n",
    "                \"max_covar_lags\": 28,                       # Max history for temporal covariates\n",
    "                \"fh\": 28,                                   # forecast horizon\n",
    "                \"train_till\": train_till,                    \n",
    "                \"test_till\": test_till,\n",
    "                \"scaling_method\": 'mean_scaling',           # use mean scaled target \n",
    "                \"interleave\": 1,                            # interleave == 1 uses all samples for training\n",
    "                \"recency_weights\": False,                   # use greater weights for more recent samples\n",
    "                \"recency_alpha\": 1,                         # controls the recency weight distribution, higher the value, greater the weightage for recent samples\n",
    "                \"hierarchical_weights\": True,               # Infers weights for each hierarchy level\n",
    "}\n",
    "\n",
    "# model config\n",
    "model_config = {\n",
    "                \"model_dim\": 64,\n",
    "                \"num_gnn_layers\": 1,\n",
    "                \"num_rnn_layers\": 2,\n",
    "                \"heads\": 1,\n",
    "                \"dropout\": 0,\n",
    "                \"device\": 'cuda',\n",
    "                \"chunk_size\": None,\n",
    "                \"batched_train\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# train config\n",
    "scheduler_params_config = {\n",
    "                            \"factor\": 0.5, \n",
    "                            \"patience\": 5, \n",
    "                            \"threshold\": 0.0001, \n",
    "                            \"min_lr\": 0.0001, \n",
    "                            \"clip_gradients\": False, \n",
    "                            \"max_norm\": 2.0, \n",
    "                            \"norm_type\": 2\n",
    "} \n",
    "\n",
    "train_config = {\n",
    "                \"lr\": 0.001,\n",
    "                \"min_epochs\": 10,\n",
    "                \"max_epochs\": 100,\n",
    "                \"patience\": 10,\n",
    "                \"model_prefix\": \"m5_statedept\",\n",
    "                \"loss_type\": 'Huber',\n",
    "                \"huber_delta\": 0.5,\n",
    "                \"use_amp\": False,\n",
    "                \"use_lr_scheduler\": True,\n",
    "                \"scheduler_params\": scheduler_params_config,\n",
    "                \"sample_weights\": True\n",
    "}    \n",
    "\n",
    "\n",
    "# infer config\n",
    "infer_config = {\n",
    "                \"infer_start\": infer_start,\n",
    "                \"infer_end\": infer_end,\n",
    "}\n",
    "\n",
    "\n",
    "# final config\n",
    "config = {\n",
    "            \"model_type\": 'GraphSeq2Seq',\n",
    "            \"working_dir\": './m5_pytorch_graphstam_workdir',\n",
    "            \"features_config\": features_config,\n",
    "            \"rolling_features\": rolling_features,\n",
    "            \"data_config\": data_config,\n",
    "            \"model_config\": model_config,\n",
    "            \"train_config\": train_config,\n",
    "            \"infer_config\": infer_config\n",
    "}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db7efc",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train one model per department\n",
    "\n",
    "for dept in df['dept_id'].unique().tolist():\n",
    "    print(\"\\ndept_id: \", dept, \"\\n\")\n",
    "    df_dept = df[df['dept_id']==dept]\n",
    "    df_dept = create_weights(df_dept)\n",
    "\n",
    "    gmlobj = graphstam.gml(config)\n",
    "    gmlobj.build(df_dept)\n",
    "    gmlobj.train()\n",
    "    forecast_df = gmlobj.infer(forecast_filepath=f\"./m5/output/m5_dept_{dept}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3f8f6",
   "metadata": {},
   "source": [
    "#### Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate all department level outputs \n",
    "\n",
    "forecast_dir = \"./m5/output\"\n",
    "\n",
    "filepathlist = []\n",
    "filenamelist = []\n",
    "\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(forecast_dir):\n",
    "    for fname in files:\n",
    "        if re.match(\"^.*.csv$\", fname):\n",
    "            filepath = os.path.join(root, fname)\n",
    "            print(filepath, i)\n",
    "            filepathlist.append(filepath)\n",
    "            filenamelist.append(fname)\n",
    "            i += 1\n",
    "            \n",
    "# Concatenate all files into a single dataframe\n",
    "out_df = pd.DataFrame()\n",
    "i=0\n",
    "for name, path in zip(filenamelist,filepathlist):\n",
    "    print(\"file: \", name, path)\n",
    "    frame = pd.read_csv(path)\n",
    "    out_df =  pd.concat([out_df, frame], axis=0)\n",
    "    i += 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast 'd' as int\n",
    "\n",
    "out_df['d'] = out_df['d'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all key levels at which forecasts were generated\n",
    "\n",
    "out_df['key_level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322edc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for submission only L12 level forecast is required\n",
    "\n",
    "out_df = out_df[out_df['key_level']=='key_store_id_item_id']\n",
    "\n",
    "# check that there are 30490 keys in the set\n",
    "out_df['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate submission file -- following hack is to allow for submission for final 28 days \n",
    "\n",
    "out_df.rename(columns={'forecast_0.5':'forecast'}, inplace=True)\n",
    "\n",
    "f_cols_dict = {}\n",
    "\n",
    "for d,i in zip(range(1942,1970),range(1,29)):\n",
    "    f_cols_dict[d] = f'F{i}'\n",
    "\n",
    "df_eval = pd.pivot_table(out_df, values='forecast', index=['id'], columns='d').reset_index()\n",
    "df_eval.rename(columns=f_cols_dict, inplace=True)\n",
    "df_eval['id'] = df_eval['id'] + '_evaluation'\n",
    "df_val = df_eval.copy() \n",
    "df_val['id'] = df_val['id'].str.replace('evaluation','validation')\n",
    "df_submit = pd.concat([df_val, df_eval], axis=0)\n",
    "df_submit['id'] = df_submit['id'].str.split('_').str[2:5].str.join(\"_\") + '_' + df_submit['id'].str.split('_').str[0:2].str.join(\"_\") + '_' + df_submit['id'].str.split('_').str[-1]\n",
    "\n",
    "df_submit.to_csv('m5_hier_dept_huber.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
